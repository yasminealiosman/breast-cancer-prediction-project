{"cells":[{"cell_type":"markdown","metadata":{"id":"Hx3WX9BoAg3h"},"source":["# Final Report & Visual Summary"]},{"cell_type":"markdown","metadata":{"id":"l-_GCZw9AjNQ"},"source":["## Introduction\n","This notebook distills our modelling journey into a visual and interpretive dashboard. We highlight performance, feature importance, and dimensionality insights- bridging technical rigor with narrative clarity.\n","\n","**Goal:** Create an interactive dashboard to explore model predictions, feature importance, and class separation. This notebook wraps up your workflow with visual storytelling.\n"]},{"cell_type":"markdown","metadata":{"id":"qUYMG_8LLWoO"},"source":["## Setup"]},{"cell_type":"code","source":["import os\n","import joblib\n","\n","def load_test_data():\n","    \"\"\"\n","    Load X_test and y_test from the correct environment path.\n","    Works in both Colab and local Windows setups.\n","    \"\"\"\n","    # Detect environment\n","    if \"COLAB_GPU\" in os.environ:  # running in Colab\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        base_path = \"/content/drive/My Drive/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models\"\n","    else:  # running locally on Windows\n","        base_path = r\"G:\\My Drive\\Portfolio\\DataSciencePortfolio\\Projects\\Breast-Cancer\\models\"\n","\n","    # Build file paths\n","    x_path = os.path.join(base_path, \"X_test.pkl\")\n","    y_path = os.path.join(base_path, \"y_test.pkl\")\n","\n","    # Check existence before loading\n","    if not os.path.exists(x_path):\n","        raise FileNotFoundError(f\"X_test.pkl not found at {x_path}\")\n","    if not os.path.exists(y_path):\n","        raise FileNotFoundError(f\"y_test.pkl not found at {y_path}\")\n","\n","    # Load with joblib\n","    X_test = joblib.load(x_path)\n","    y_test = joblib.load(y_path)\n","\n","    return X_test, y_test"],"metadata":{"id":"bYPUAlSy_EaF","executionInfo":{"status":"ok","timestamp":1765568197648,"user_tz":0,"elapsed":59,"user":{"displayName":"Yasmine Ali-Osman","userId":"01601773200894379900"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125,"status":"ok","timestamp":1765568195652,"user":{"displayName":"Yasmine Ali-Osman","userId":"01601773200894379900"},"user_tz":0},"id":"hsgjeHU8RHQw","outputId":"406ca839-2f0d-40f4-e9b8-1e41ca0aaa70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting breast_cancer_dashboard.py\n"]}],"source":["%%writefile breast_cancer_dashboard.py\n","\n","# Breast Cancer Prediction Dashboard (Streamlit)\n","\n","import os\n","import json\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import streamlit as st\n","\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from sklearn.metrics import (\n","    roc_auc_score, roc_curve, confusion_matrix,\n","    precision_score, recall_score, f1_score, accuracy_score,\n","    brier_score_loss\n",")\n","from sklearn.calibration import calibration_curve\n","\n","# -------------------------\n","# App config\n","# -------------------------\n","st.set_page_config(page_title=\"Breast Cancer Prediction Dashboard\", layout=\"wide\")\n","st.title(\"Breast Cancer Prediction Dashboard\")\n","st.caption(\"This dashboard is for research/decision support, not a substitute for medical diagnosis.\")\n","\n","# -------------------------\n","# Paths\n","# -------------------------\n","DRIVE_ROOT = \"/content/drive/My Drive/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer\"\n","MODELS_DIR = os.path.join(DRIVE_ROOT, \"models\")\n","DASHBOARD_DIR = os.path.join(DRIVE_ROOT, \"dashboard\")\n","ARTIFACTS_DIR = os.path.join(DASHBOARD_DIR, \"artifacts\")\n","EXPORTS_DIR = os.path.join(DASHBOARD_DIR, \"exports\")\n","os.makedirs(DASHBOARD_DIR, exist_ok=True)\n","os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n","os.makedirs(EXPORTS_DIR, exist_ok=True)\n","\n","# -------------------------\n","# Utilities\n","# -------------------------\n","def safe_load(path, kind=\"pickle\"):\n","    try:\n","        if kind == \"pickle\":\n","            return joblib.load(path)\n","        elif kind == \"csv\":\n","            return pd.read_csv(path)\n","        elif kind == \"json\":\n","            with open(path, \"r\") as f:\n","                return json.load(f)\n","    except Exception as e:\n","        st.warning(f\"Missing or unreadable file: {path} ({e})\")\n","        return None\n","\n","def save_fig(fig, filename):\n","    fp = os.path.join(ARTIFACTS_DIR, filename)\n","    fig.savefig(fp, bbox_inches=\"tight\", dpi=150)\n","    return fp\n","\n","def specificity_score(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn = cm[0,0]\n","    fp = cm[0,1]\n","    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n","\n","def compute_ppv(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tp = cm[1,1]\n","    fp = cm[0,1]\n","    return tp / (tp + fp) if (tp + fp) > 0 else np.nan\n","\n","def compute_npv(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn = cm[0,0]\n","    fn = cm[1,0]\n","    return tn / (tn + fn) if (tn + fn) > 0 else np.nan\n","\n","def compute_brier(y_true, y_prob):\n","    return brier_score_loss(y_true, y_prob)\n","\n","def plot_confusion_matrix(y_true, y_pred, title=\"Confusion matrix\"):\n","    cm = confusion_matrix(y_true, y_pred)\n","    fig, ax = plt.subplots()\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False)\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"Actual\")\n","    ax.set_title(title)\n","    ax.set_xticklabels([\"Benign\", \"Malignant\"])\n","    ax.set_yticklabels([\"Benign\", \"Malignant\"], rotation=0)\n","    return fig\n","\n","def plot_roc_curves(curves, title=\"ROC curves\"):\n","    fig, ax = plt.subplots()\n","    ax.plot([0,1],[0,1], \"k--\", label=\"Chance\")\n","    for name, (fpr, tpr, auc, color) in curves.items():\n","        ax.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\", color=color)\n","    ax.set_xlabel(\"False Positive Rate\")\n","    ax.set_ylabel(\"True Positive Rate\")\n","    ax.set_title(title)\n","    ax.legend(loc=\"lower right\")\n","    return fig\n","\n","def decision_curve(y_true, y_prob, thresholds=np.linspace(0.01, 0.99, 50)):\n","    \"\"\"\n","    Net benefit = (TP/n) - (FP/n) * (threshold / (1 - threshold))\n","    \"\"\"\n","    n = len(y_true)\n","    rows = []\n","    for thr in thresholds:\n","        y_pred = (y_prob >= thr).astype(int)\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","        net_benefit = (tp/n) - (fp/n) * (thr / (1 - thr))\n","        rows.append({\"threshold\": thr, \"net_benefit\": net_benefit})\n","    return pd.DataFrame(rows)\n","\n","# -------------------------\n","# Sidebar configuration\n","# -------------------------\n","st.sidebar.header(\"Configuration\")\n","\n","# Only LR and GB\n","expected_models = {\n","    \"LR\": (\"model_lr.pkl\", \"threshold_lr.pkl\"),\n","    \"GB\": (\"model_gb.pkl\", \"threshold_gb.pkl\"),\n","}\n","\n","# Paths\n","eval_csv_path = st.sidebar.text_input(\n","    \"Evaluation summary CSV\",\n","    os.path.join(DASHBOARD_DIR, \"evaluation_summary.csv\")\n",")\n","x_test_path = st.sidebar.text_input(\n","    \"X_test CSV (features)\",\n","    os.path.join(DASHBOARD_DIR, \"X_test.csv\")\n",")\n","y_test_path = st.sidebar.text_input(\n","    \"y_test CSV (labels)\",\n","    os.path.join(DASHBOARD_DIR, \"y_test.csv\")\n",")\n","\n","# Model selection\n","selected_models = st.sidebar.multiselect(\n","    \"Models to include\",\n","    options=list(expected_models.keys()),\n","    default=list(expected_models.keys())\n",")\n","\n","# Threshold tuning sliders (per model)\n","st.sidebar.subheader(\"Threshold tuning\")\n","threshold_controls = {}\n","for m in selected_models:\n","    default_thr_obj = safe_load(os.path.join(MODELS_DIR, expected_models[m][1]), kind=\"pickle\")\n","    default_thr = float(default_thr_obj) if default_thr_obj is not None else 0.5\n","    threshold_controls[m] = st.sidebar.slider(\n","        f\"{m} threshold (default={default_thr:.2f})\",\n","        0.0, 1.0, default_thr, 0.01\n","    )\n","\n","# Interpretability mode (no SHAP)\n","interpret_mode = st.sidebar.radio(\n","    \"Interpretability mode\",\n","    [\"Feature Importance\", \"Calibration\"],\n","    index=0\n",")\n","\n","# Calibration toggle\n","show_calibration = st.sidebar.checkbox(\"Show calibration curves\", value=True)\n","\n","# -------------------------\n","# Load data\n","# -------------------------\n","X_test = safe_load(x_test_path, kind=\"csv\")\n","y_test = safe_load(y_test_path, kind=\"csv\")\n","if isinstance(y_test, pd.DataFrame):\n","    y_test = y_test.iloc[:, 0]\n","elif y_test is None:\n","    st.error(\"Test labels not found. Please provide y_test.csv.\")\n","if X_test is None:\n","    st.error(\"Test features not found. Please provide X_test.csv.\")\n","if X_test is None or y_test is None:\n","    st.stop()\n","\n","st.success(\"✅ Test data loaded\")\n","\n","# Load evaluation summary and filter to LR/GB\n","eval_df = safe_load(eval_csv_path, kind=\"csv\")\n","if isinstance(eval_df, pd.DataFrame) and not eval_df.empty and \"Model\" in eval_df.columns:\n","    eval_df = eval_df[eval_df[\"Model\"].isin([\"LR\", \"GB\"])].copy()\n","    st.sidebar.success(\"✅ Evaluation summary loaded\")\n","    st.sidebar.dataframe(eval_df.head())\n","else:\n","    st.sidebar.warning(\"⚠️ Evaluation summary not found or empty\")\n","\n","# -------------------------\n","# Load models\n","# -------------------------\n","models = {}\n","thresholds = {}\n","load_msgs = []\n","for key in selected_models:\n","    mfile, tfile = expected_models[key]\n","    model = safe_load(os.path.join(MODELS_DIR, mfile), kind=\"pickle\")\n","    thr_obj = safe_load(os.path.join(MODELS_DIR, tfile), kind=\"pickle\")\n","    thr_val = threshold_controls[key] if key in threshold_controls else (float(thr_obj) if thr_obj is not None else 0.5)\n","    if model is not None:\n","        models[key] = model\n","        thresholds[key] = float(thr_val)\n","        load_msgs.append(f\"✅ {key} loaded (threshold={thr_val:.3f})\")\n","    else:\n","        load_msgs.append(f\"⚠️ {key} missing\")\n","\n","st.subheader(\"Model load status\")\n","st.write(\"\\n\".join(load_msgs))\n","if not models:\n","    st.error(\"No models loaded. Please check your configuration.\")\n","    st.stop()\n","\n","# -------------------------\n","# Overview\n","# -------------------------\n","st.subheader(\"Overview\")\n","n_test = len(X_test)\n","models_list = \", \".join(models.keys())\n","\n","acc_range_txt = \"—\"\n","if isinstance(eval_df, pd.DataFrame) and \"Accuracy\" in eval_df.columns and not eval_df.empty:\n","    acc_min = eval_df[\"Accuracy\"].min()\n","    acc_max = eval_df[\"Accuracy\"].max()\n","    acc_range_txt = f\"{acc_min:.1f}%\" if acc_min == acc_max else f\"{acc_min:.1f}%–{acc_max:.1f}%\"\n","\n","c1, c2, c3 = st.columns(3)\n","with c1:\n","    st.metric(label=\"Test set size\", value=n_test)\n","with c2:\n","    st.metric(label=\"Models compared\", value=models_list)\n","with c3:\n","    st.metric(label=\"Accuracy range\", value=acc_range_txt)\n","\n","st.markdown(\"**Interpretability note:** LR provides interpretable coefficients (risk factors); GB provides feature importance and is evaluated with calibration curves for probability reliability.\")\n","\n","# -------------------------\n","# Model comparison table + bar plot\n","# -------------------------\n","st.subheader(\"Model comparison\")\n","if isinstance(eval_df, pd.DataFrame) and not eval_df.empty:\n","    cols_expected = [c for c in [\"Model\",\"Precision\",\"Recall\",\"F1\",\"Specificity\",\"ROC_AUC\",\"Accuracy\"] if c in eval_df.columns]\n","    if cols_expected:\n","        st.dataframe(eval_df[cols_expected], width=\"stretch\")\n","        show_cols = [c for c in [\"Recall\",\"Specificity\",\"F1\",\"ROC_AUC\"] if c in eval_df.columns]\n","        if show_cols:\n","            fig_bar, ax = plt.subplots()\n","            eval_df.set_index(\"Model\")[show_cols].plot(kind=\"bar\", ax=ax, color=[\"#1f77b4\",\"#2ca02c\",\"#9467bd\",\"#ff7f0e\"])\n","            ax.set_title(\"Balanced metrics comparison (Recall, Specificity, F1, ROC_AUC)\")\n","            ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n","            st.pyplot(fig_bar)\n","            save_fig(fig_bar, \"comparison_balanced_metrics.png\")\n","else:\n","    st.info(\"No evaluation_summary.csv found. Provide it to view the comparison table and charts.\")\n","\n","# -------------------------\n","# ROC curves overlay\n","# -------------------------\n","st.subheader(\"ROC curves\")\n","roc_curves = {}\n","color_map = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n","for name, model in models.items():\n","    try:\n","        proba = model.predict_proba(X_test)[:,1]\n","        auc = roc_auc_score(y_test, proba)\n","        fpr, tpr, _ = roc_curve(y_test, proba)\n","        roc_curves[name] = (fpr, tpr, auc, color_map.get(name, None))\n","    except Exception as e:\n","        st.warning(f\"Skipping ROC for {name}: {e}\")\n","\n","if roc_curves:\n","    fig_roc = plot_roc_curves(roc_curves, title=\"ROC curves by model (AUC in legend)\")\n","    st.pyplot(fig_roc)\n","    save_fig(fig_roc, \"roc_overlay.png\")\n","\n","# -------------------------\n","# Calibration curves\n","# -------------------------\n","if show_calibration:\n","    st.subheader(\"Calibration curves\")\n","    cal_cols = st.columns(min(len(models), 3))\n","    for idx, (name, model) in enumerate(models.items()):\n","        try:\n","            if hasattr(model, \"predict_proba\"):\n","                y_prob = model.predict_proba(X_test)[:, 1]\n","                prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n","\n","                fig_cal, ax = plt.subplots()\n","                ax.plot(prob_pred, prob_true, marker='o', label=name, color=color_map.get(name, None))\n","                ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n","                ax.set_xlabel(\"Predicted probability\")\n","                ax.set_ylabel(\"True probability\")\n","                ax.set_title(f\"Calibration Curve - {name}\")\n","                ax.legend()\n","                with cal_cols[idx % len(cal_cols)]:\n","                    st.pyplot(fig_cal)\n","                save_fig(fig_cal, f\"calibration_{name.lower()}.png\")\n","        except Exception as e:\n","            st.warning(f\"Skipping calibration for {name}: {e}\")\n","\n","# -------------------------\n","# Confusion matrices\n","# -------------------------\n","st.subheader(\"Confusion matrices\")\n","cm_cols = st.columns(min(len(models), 4))\n","for idx, (name, model) in enumerate(models.items()):\n","    thr = thresholds.get(name, 0.5)\n","    try:\n","        proba = model.predict_proba(X_test)[:,1]\n","        y_pred = (proba >= thr).astype(int)\n","        fig_cm = plot_confusion_matrix(y_test, y_pred, title=f\"{name} (thr={thr:.3f})\")\n","        with cm_cols[idx % len(cm_cols)]:\n","            st.pyplot(fig_cm)\n","        save_fig(fig_cm, f\"cm_{name.lower()}.png\")\n","    except Exception as e:\n","        st.warning(f\"Skipping CM for {name}: {e}\")\n","\n","# -------------------------\n","# Threshold tuning metrics (add PPV, NPV, Brier)\n","# -------------------------\n","st.subheader(\"Threshold tuning metrics\")\n","metrics_rows = []\n","for name, model in models.items():\n","    thr = thresholds.get(name, 0.5)\n","    try:\n","        proba = model.predict_proba(X_test)[:,1]\n","        y_pred = (proba >= thr).astype(int)\n","        prec = precision_score(y_test, y_pred, zero_division=0)\n","        rec = recall_score(y_test, y_pred, zero_division=0)\n","        f1 = f1_score(y_test, y_pred, zero_division=0)\n","        acc = accuracy_score(y_test, y_pred)\n","        spec = specificity_score(y_test, y_pred)\n","        auc = roc_auc_score(y_test, proba)\n","        ppv = compute_ppv(y_test, y_pred)\n","        npv = compute_npv(y_test, y_pred)\n","        brier = compute_brier(y_test, proba)\n","        metrics_rows.append({\n","            \"Model\": name, \"Threshold\": thr,\n","            \"Precision\": round(prec,3), \"Recall\": round(rec,3),\n","            \"F1\": round(f1,3), \"Specificity\": round(spec,3),\n","            \"Accuracy\": round(acc,3), \"ROC_AUC\": round(auc,3),\n","            \"PPV\": round(ppv,3), \"NPV\": round(npv,3),\n","            \"Brier\": round(brier,3)\n","        })\n","    except Exception as e:\n","        st.warning(f\"Skipping metrics for {name}: {e}\")\n","\n","if metrics_rows:\n","    metrics_df = pd.DataFrame(metrics_rows)\n","    st.dataframe(metrics_df, width=\"stretch\")\n","    metrics_df.to_csv(os.path.join(ARTIFACTS_DIR, \"threshold_tuning_metrics.csv\"), index=False)\n","\n","# -------------------------\n","# Interpretability (LR coefficients, GB feature importance)\n","# -------------------------\n","st.header(\"Interpretability\")\n","\n","# LR coefficients from pipeline final step\n","if \"LR\" in models:\n","    try:\n","        lr = models[\"LR\"]\n","        # Extract final estimator from pipeline if needed\n","        final_lr = lr\n","        if hasattr(lr, \"named_steps\"):\n","            final_lr = lr.named_steps.get(\"clf\", lr)\n","        if hasattr(final_lr, \"coef_\"):\n","            coef = final_lr.coef_.ravel()\n","            feat_names = list(X_test.columns)[:len(coef)]\n","            lr_df = pd.DataFrame({\"Feature\": feat_names, \"Coefficient\": coef})\n","            lr_df[\"Impact\"] = np.where(lr_df[\"Coefficient\"] >= 0, \"Positive\", \"Negative\")\n","            lr_df = lr_df.sort_values(\"Coefficient\", ascending=False)\n","\n","            st.markdown(\"**Logistic Regression coefficients (risk factors):**\")\n","            st.dataframe(lr_df, width=\"stretch\")\n","\n","            fig_lr, ax = plt.subplots(figsize=(7,5))\n","            sns.barplot(data=lr_df, x=\"Coefficient\", y=\"Feature\", hue=\"Impact\", dodge=False, ax=ax, palette={\"Positive\":\"crimson\",\"Negative\":\"steelblue\"})\n","            ax.set_title(\"LR feature impacts (Positive = higher malignancy risk)\")\n","            st.pyplot(fig_lr)\n","            save_fig(fig_lr, \"lr_coefficients.png\")\n","        else:\n","            st.info(\"LR coefficients not accessible on the loaded object.\")\n","    except Exception as e:\n","        st.warning(f\"LR interpretability error: {e}\")\n","\n","# GB feature importance from pipeline final step\n","if \"GB\" in models:\n","    try:\n","        gb = models[\"GB\"]\n","        final_gb = gb\n","        if hasattr(gb, \"named_steps\"):\n","            final_gb = gb.named_steps.get(\"clf\", gb)\n","        if hasattr(final_gb, \"feature_importances_\"):\n","            importances = final_gb.feature_importances_\n","            feats = list(X_test.columns)[:len(importances)]\n","            fi_df = pd.DataFrame({\"Feature\": feats, \"Importance\": importances}).sort_values(\"Importance\", ascending=False)\n","\n","            st.markdown(\"**Gradient Boosting feature importance:**\")\n","            st.dataframe(fi_df.head(20), width=\"stretch\")\n","\n","            fig_fi, ax = plt.subplots(figsize=(7,5))\n","            sns.barplot(data=fi_df.head(15), x=\"Importance\", y=\"Feature\", ax=ax, color=\"#ff7f0e\")\n","            ax.set_title(\"GB feature importance (top 15)\")\n","            st.pyplot(fig_fi)\n","            save_fig(fig_fi, \"gb_feature_importance.png\")\n","        else:\n","            st.info(\"GB feature importances not accessible on the loaded object.\")\n","    except Exception as e:\n","        st.warning(f\"GB interpretability error: {e}\")\n","\n","# -------------------------\n","# Deployment / reuse (export + batch scoring)\n","# -------------------------\n","st.subheader(\"Deployment / reuse\")\n","\n","# Export selected model + threshold to Drive\n","model_to_export = st.selectbox(\"Select model to export\", options=list(models.keys()))\n","if st.button(\"Export model + threshold bundle\"):\n","    bundle = {\n","        \"model_path\": os.path.join(MODELS_DIR, expected_models[model_to_export][0]),\n","        \"model_type\": type(models[model_to_export]).__name__,\n","        \"threshold\": thresholds.get(model_to_export, 0.5),\n","        \"features\": list(X_test.columns)\n","    }\n","    export_fp = os.path.join(EXPORTS_DIR, f\"{model_to_export.lower()}_bundle.json\")\n","    with open(export_fp, \"w\") as f:\n","        json.dump(bundle, f, indent=2)\n","    st.success(f\"Exported bundle: {export_fp}\")\n","\n","# Batch scoring interface (upload CSV and score)\n","st.markdown(\"**Batch scoring:** Upload a CSV of new patient data to get predictions.\")\n","uploaded = st.file_uploader(\"Upload CSV for scoring\", type=[\"csv\"])\n","if uploaded is not None:\n","    try:\n","        new_df = pd.read_csv(uploaded)\n","        # Align columns with training/test features\n","        missing_cols = set(X_test.columns) - set(new_df.columns)\n","        if missing_cols:\n","            st.warning(f\"Uploaded data missing columns: {missing_cols}\")\n","            for col in missing_cols:\n","                new_df[col] = 0.0\n","        new_df = new_df[X_test.columns]  # enforce column order\n","\n","        pick_model = st.selectbox(\"Model for batch scoring\", options=list(models.keys()), key=\"batch_model\")\n","        thr = thresholds.get(pick_model, 0.5)\n","        mdl = models[pick_model]\n","        proba = mdl.predict_proba(new_df)[:,1]\n","        pred = (proba >= thr).astype(int)\n","\n","        out = new_df.copy()\n","        out[\"probability\"] = proba\n","        out[\"prediction\"] = pred\n","        st.dataframe(out.head(30), width=\"stretch\")\n","\n","        # Save batch results\n","        out_fp = os.path.join(ARTIFACTS_DIR, f\"batch_{pick_model.lower()}_results.csv\")\n","        out.to_csv(out_fp, index=False)\n","        st.success(f\"Saved batch results: {out_fp}\")\n","    except Exception as e:\n","        st.error(f\"Batch scoring failed: {e}\")\n","\n","# -------------------------\n","# Interactive prediction (single case) with clinical reliability metrics\n","# -------------------------\n","st.subheader(\"Interactive prediction\")\n","feature_cols = list(X_test.columns)\n","numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(X_test[c])]\n","use_cols = numeric_cols[:6] if len(numeric_cols) >= 2 else feature_cols\n","\n","with st.form(\"single_prediction\"):\n","    inputs = {}\n","    for c in use_cols:\n","        default_val = float(np.nanmean(X_test[c])) if c in X_test.columns else 0.0\n","        inputs[c] = st.number_input(c, value=default_val)\n","    model_choice = st.selectbox(\"Model\", options=list(models.keys()), key=\"single_model\")\n","    submitted = st.form_submit_button(\"Predict\")\n","\n","if submitted:\n","    try:\n","        df_in = pd.DataFrame([inputs])\n","        model = models[model_choice]\n","        thr = thresholds.get(model_choice, 0.5)\n","        proba = model.predict_proba(df_in)[:,1][0]\n","        pred = int(proba >= thr)\n","\n","        # Reliability metrics from test set\n","        y_prob_test = model.predict_proba(X_test)[:,1]\n","        y_pred_test = (y_prob_test >= thr).astype(int)\n","        ppv = compute_ppv(y_test, y_pred_test)\n","        npv = compute_npv(y_test, y_pred_test)\n","        brier = compute_brier(y_test, y_prob_test)\n","\n","        st.success(f\"Probability: {proba:.3f} | Prediction: {'Malignant' if pred==1 else 'Benign'} (thr={thr:.3f})\")\n","        st.info(f\"PPV: {ppv:.3f} | NPV: {npv:.3f} | Brier score: {brier:.3f}\")\n","    except Exception as e:\n","        st.error(f\"Prediction failed: {e}\")\n","\n","# -------------------------\n","# Decision Curve Analysis (distinct colors + CSV export)\n","# -------------------------\n","st.header(\"Decision Curve Analysis\")\n","\n","dca_colors = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n","\n","dca_frames = []\n","fig_dca, ax = plt.subplots()\n","for name, model in models.items():\n","    try:\n","        y_prob = model.predict_proba(X_test)[:,1]\n","        dca_df = decision_curve(y_test, y_prob)\n","        dca_df[\"Model\"] = name\n","        dca_frames.append(dca_df)\n","        ax.plot(dca_df[\"threshold\"], dca_df[\"net_benefit\"], label=name, color=dca_colors.get(name, None), linewidth=2)\n","    except Exception as e:\n","        st.warning(f\"Skipping DCA for {name}: {e}\")\n","\n","# Baseline strategies\n","prevalence = y_test.mean()\n","thr_grid = np.linspace(0.01, 0.99, 50)\n","treat_all = [prevalence - (1 - prevalence) * (thr / (1 - thr)) for thr in thr_grid]\n","treat_none = [0 for _ in thr_grid]\n","ax.plot(thr_grid, treat_all, linestyle=\"--\", color=\"black\", label=\"Treat All\")\n","ax.plot(thr_grid, treat_none, linestyle=\":\", color=\"gray\", label=\"Treat None\")\n","\n","ax.set_xlabel(\"Threshold probability\")\n","ax.set_ylabel(\"Net benefit\")\n","ax.set_title(\"Decision Curve Analysis\")\n","ax.legend(loc=\"best\")\n","st.pyplot(fig_dca)\n","save_fig(fig_dca, \"decision_curve.png\")\n","\n","# Export DCA CSV (combined)\n","if dca_frames:\n","    dca_all = pd.concat(dca_frames, ignore_index=True)\n","    dca_fp = os.path.join(ARTIFACTS_DIR, \"decision_curve.csv\")\n","    dca_all.to_csv(dca_fp, index=False)\n","    st.success(f\"Saved DCA results: {dca_fp}\")\n","    st.dataframe(dca_all.head(20), width=\"stretch\")\n","\n","# -------------------------\n","# Artifacts saved to Drive\n","# -------------------------\n","st.subheader(\"Artifacts saved to Drive\")\n","artifact_list = []\n","for root, _, files in os.walk(ARTIFACTS_DIR):\n","    for f in files:\n","        artifact_list.append(os.path.join(root, f))\n","\n","st.write(f\"Artifacts directory: {ARTIFACTS_DIR}\")\n","if artifact_list:\n","    for fp in artifact_list:\n","        name = os.path.basename(fp)\n","        with open(fp, \"rb\") as fh:\n","            st.download_button(label=f\"Download {name}\", data=fh, file_name=name)\n","\n","    # Download all artifacts as ZIP\n","    import zipfile\n","    zip_fp = os.path.join(ARTIFACTS_DIR, \"artifacts_bundle.zip\")\n","    with zipfile.ZipFile(zip_fp, \"w\") as zf:\n","        for fp in artifact_list:\n","            zf.write(fp, os.path.basename(fp))\n","    with open(zip_fp, \"rb\") as fh:\n","        st.download_button(label=\"Download all artifacts (ZIP)\", data=fh, file_name=\"artifacts_bundle.zip\")\n","else:\n","    st.info(\"No artifacts saved yet in the session.\")"]},{"cell_type":"markdown","metadata":{"id":"dhQAUs-Udfoc"},"source":["# Deployment Guide\n","\n","This section explains how to reuse models outside the dashboard and apply them to new patient data.\n","\n","\n","\n","## 1. Export Model + Threshold Bundle\n","- **Purpose:** Save a trained model with its tuned threshold and feature list for reuse.  \n","- **Contents of the bundle:**\n","  - Model path (location of the serialized model file)\n","  - Model type (e.g., Logistic Regression, Random Forest)\n","  - Tuned threshold value\n","  - Feature list (columns expected in input data)\n","- **Usage:** Exported bundles can be loaded in other environments (e.g., Colab, Streamlit apps) to ensure consistent predictions.\n","\n","\n","\n","## 2. Batch Scoring\n","- **Purpose:** Score multiple new patient records at once.  \n","- **Steps:**\n","  1. Upload a CSV file containing patient features.  \n","  2. Select the model to use for scoring.  \n","  3. The dashboard outputs:\n","     - Predicted probability of malignancy\n","     - Final prediction (Benign/Malignant) based on the tuned threshold\n","  4. Results are saved as a CSV in the artifacts directory for reproducibility.\n","- **Note:** Uploaded CSV must align with the training feature set. Missing columns are automatically filled with default values.\n","\n","\n","\n","## 3. Interactive Prediction (Single Case)\n","- **Purpose:** Test the model on a single patient case interactively.  \n","- **Steps:**\n","  1. Enter values for selected features (defaults are pre-filled with dataset averages).  \n","  2. Choose a model and click **Predict**.  \n","  3. The dashboard displays:\n","     - Probability of malignancy\n","     - Final prediction (Benign/Malignant)\n","- **Optional:** If SHAP is enabled, a local explanation plot shows which features most influenced the prediction.\n","\n","\n","\n","## 4. Artifacts Management\n","- **Purpose:** Ensure reproducibility and easy sharing of results.  \n","- **Artifacts saved include:**\n","  - ROC curves\n","  - Confusion matrices\n","  - Calibration curves\n","  - SHAP plots\n","  - Threshold tuning metrics CSV\n","  - Batch scoring results\n","- **Download options:**\n","  - Individual files (plots, CSVs, models)\n","  - Full ZIP bundle containing all artifacts\n","  \n","  \n","\n"," **Key Takeaway:**  \n","This deployment workflow ensures that models trained in the dashboard can be exported, reused, and explained consistently. Batch scoring supports large datasets, while interactive prediction enables case-by-case interpretability."]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEv/5I4cxC0zgN0cWkN+Kf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}