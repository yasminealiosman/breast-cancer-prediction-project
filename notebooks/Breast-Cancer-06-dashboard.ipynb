{"cells":[{"cell_type":"markdown","metadata":{"id":"Hx3WX9BoAg3h"},"source":["# Final Report & Visual Summary"]},{"cell_type":"markdown","metadata":{"id":"l-_GCZw9AjNQ"},"source":["## Introduction\n","This notebook distills our modelling journey into a visual and interpretive dashboard. We highlight performance, feature importance, and dimensionality insights- bridging technical rigor with narrative clarity.\n","\n","**Goal:** Create an interactive dashboard to explore model predictions, feature importance, and class separation. This notebook wraps up your workflow with visual storytelling.\n"]},{"cell_type":"markdown","metadata":{"id":"qUYMG_8LLWoO"},"source":["## Setup"]},{"cell_type":"code","source":["import os\n","import joblib\n","import pandas as pd\n","\n","def load_test_data(file_type=\"csv\"):\n","    \"\"\"\n","    Load X_test and y_test from repo-relative paths.\n","    Works in Streamlit Cloud, Colab, and local setups.\n","\n","    Parameters\n","    ----------\n","    file_type : str\n","        \"csv\" (default) if you saved test data as CSVs,\n","        \"pkl\" if you prefer joblib pickles.\n","\n","    Returns\n","    -------\n","    X_test, y_test : pandas.DataFrame or numpy array\n","    \"\"\"\n","    if file_type == \"csv\":\n","        x_path = os.path.join(\"dashboard\", \"X_test.csv\")\n","        y_path = os.path.join(\"dashboard\", \"y_test.csv\")\n","        if not os.path.exists(x_path) or not os.path.exists(y_path):\n","            raise FileNotFoundError(\"CSV test files not found in dashboard/\")\n","        X_test = pd.read_csv(x_path)\n","        y_test = pd.read_csv(y_path)\n","    elif file_type == \"pkl\":\n","        x_path = os.path.join(\"dashboard\", \"X_test.pkl\")\n","        y_path = os.path.join(\"dashboard\", \"y_test.pkl\")\n","        if not os.path.exists(x_path) or not os.path.exists(y_path):\n","            raise FileNotFoundError(\"Pickle test files not found in dashboard/\")\n","        X_test = joblib.load(x_path)\n","        y_test = joblib.load(y_path)\n","    else:\n","        raise ValueError(\"file_type must be 'csv' or 'pkl'\")\n","\n","    return X_test, y_test"],"metadata":{"id":"qWLDE6Uwd3HE","executionInfo":{"status":"ok","timestamp":1765578977222,"user_tz":0,"elapsed":6,"user":{"displayName":"Yasmine Ali-Osman","userId":"01601773200894379900"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1765578975545,"user":{"displayName":"Yasmine Ali-Osman","userId":"01601773200894379900"},"user_tz":0},"id":"hsgjeHU8RHQw","outputId":"b4a41b52-22c3-4366-d278-38cd83cf4570"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting breast_cancer_dashboard.py\n"]}],"source":["%%writefile breast_cancer_dashboard.py\n","\n","# Breast Cancer Prediction Dashboard (Streamlit)\n","import os\n","import json\n","import joblib\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import streamlit as st\n","\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from sklearn.metrics import (\n","    roc_auc_score, roc_curve, confusion_matrix,\n","    precision_score, recall_score, f1_score, accuracy_score,\n","    brier_score_loss\n",")\n","from sklearn.calibration import calibration_curve\n","\n","# -------------------------\n","# App config\n","# -------------------------\n","st.set_page_config(page_title=\"Breast Cancer Prediction Dashboard\", layout=\"wide\")\n","st.title(\"Breast Cancer Prediction Dashboard\")\n","st.caption(\"This dashboard is for research/decision support, not a substitute for medical diagnosis.\")\n","\n","# -------------------------\n","# Paths (repo-relative)\n","# -------------------------\n","MODELS_DIR = \"models\"\n","DASHBOARD_DIR = \"dashboard\"\n","ARTIFACTS_DIR = os.path.join(DASHBOARD_DIR, \"artifacts\")\n","EXPORTS_DIR = os.path.join(DASHBOARD_DIR, \"exports\")\n","\n","# -------------------------\n","# Utilities\n","# -------------------------\n","def safe_load(path, kind=\"pickle\"):\n","    \"\"\"Load artifacts safely with Streamlit warnings if missing.\"\"\"\n","    try:\n","        if kind == \"pickle\":\n","            return joblib.load(path)\n","        elif kind == \"csv\":\n","            return pd.read_csv(path)\n","        elif kind == \"json\":\n","            with open(path, \"r\") as f:\n","                return json.load(f)\n","    except Exception as e:\n","        st.warning(f\"Missing or unreadable file: {path} ({e})\")\n","        return None\n","\n","def specificity_score(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp = cm[0,0], cm[0,1]\n","    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n","\n","def compute_ppv(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tp, fp = cm[1,1], cm[0,1]\n","    return tp / (tp + fp) if (tp + fp) > 0 else np.nan\n","\n","def compute_npv(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fn = cm[0,0], cm[1,0]\n","    return tn / (tn + fn) if (tn + fn) > 0 else np.nan\n","\n","def compute_brier(y_true, y_prob):\n","    return brier_score_loss(y_true, y_prob)\n","\n","def plot_confusion_matrix(y_true, y_pred, title=\"Confusion matrix\"):\n","    cm = confusion_matrix(y_true, y_pred)\n","    fig, ax = plt.subplots()\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False)\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"Actual\")\n","    ax.set_title(title)\n","    ax.set_xticklabels([\"Benign\", \"Malignant\"])\n","    ax.set_yticklabels([\"Benign\", \"Malignant\"], rotation=0)\n","    return fig\n","\n","def plot_roc_curves(curves, title=\"ROC curves\"):\n","    fig, ax = plt.subplots()\n","    ax.plot([0,1],[0,1], \"k--\", label=\"Chance\")\n","    for name, (fpr, tpr, auc, color) in curves.items():\n","        ax.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\", color=color)\n","    ax.set_xlabel(\"False Positive Rate\")\n","    ax.set_ylabel(\"True Positive Rate\")\n","    ax.set_title(title)\n","    ax.legend(loc=\"lower right\")\n","    return fig\n","\n","def decision_curve(y_true, y_prob, thresholds=np.linspace(0.01, 0.99, 50)):\n","    \"\"\"\n","    Net benefit = (TP/n) - (FP/n) * (threshold / (1 - threshold))\n","    \"\"\"\n","    n = len(y_true)\n","    rows = []\n","    for thr in thresholds:\n","        y_pred = (y_prob >= thr).astype(int)\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","        net_benefit = (tp/n) - (fp/n) * (thr / (1 - thr))\n","        rows.append({\"threshold\": thr, \"net_benefit\": net_benefit})\n","    return pd.DataFrame(rows)\n","\n","\n","\n","\n","# -------------------------\n","# Sidebar configuration\n","# -------------------------\n","st.sidebar.header(\"Configuration\")\n","\n","# Expected models (Logistic Regression and Gradient Boosting)\n","expected_models = {\n","    \"LR\": (\"model_lr.pkl\", \"threshold_lr.pkl\"),\n","    \"GB\": (\"model_gb.pkl\", \"threshold_gb.pkl\"),\n","}\n","\n","# Paths (repo-relative defaults, editable in sidebar)\n","eval_csv_path = st.sidebar.text_input(\n","    \"Evaluation summary CSV\",\n","    os.path.join(DASHBOARD_DIR, \"evaluation_summary.csv\")\n",")\n","x_test_path = st.sidebar.text_input(\n","    \"X_test CSV (features)\",\n","    os.path.join(DASHBOARD_DIR, \"X_test.csv\")\n",")\n","y_test_path = st.sidebar.text_input(\n","    \"y_test CSV (labels)\",\n","    os.path.join(DASHBOARD_DIR, \"y_test.csv\")\n",")\n","\n","# Model selection\n","selected_models = st.sidebar.multiselect(\n","    \"Models to include\",\n","    options=list(expected_models.keys()),\n","    default=list(expected_models.keys())\n",")\n","\n","# Threshold tuning sliders (per model)\n","st.sidebar.subheader(\"Threshold tuning\")\n","threshold_controls = {}\n","for m in selected_models:\n","    default_thr_obj = safe_load(os.path.join(MODELS_DIR, expected_models[m][1]), kind=\"pickle\")\n","    default_thr = float(default_thr_obj) if default_thr_obj is not None else 0.5\n","    threshold_controls[m] = st.sidebar.slider(\n","        f\"{m} threshold (default={default_thr:.2f})\",\n","        0.0, 1.0, default_thr, 0.01\n","    )\n","\n","# Interpretability mode\n","interpret_mode = st.sidebar.radio(\n","    \"Interpretability mode\",\n","    [\"Feature Importance\", \"Calibration\"],\n","    index=0\n",")\n","\n","# Calibration toggle\n","show_calibration = st.sidebar.checkbox(\"Show calibration curves\", value=True)\n","\n","# -------------------------\n","# Load test data\n","# -------------------------\n","X_test = safe_load(x_test_path, kind=\"csv\")\n","y_test = safe_load(y_test_path, kind=\"csv\")\n","\n","if isinstance(y_test, pd.DataFrame):\n","    y_test = y_test.iloc[:, 0]\n","elif y_test is None:\n","    st.error(\"Test labels not found. Please provide y_test.csv.\")\n","\n","if X_test is None:\n","    st.error(\"Test features not found. Please provide X_test.csv.\")\n","\n","if X_test is None or y_test is None:\n","    st.stop()\n","\n","st.success(\"✅ Test data loaded\")\n","\n","# -------------------------\n","# Load evaluation summary\n","# -------------------------\n","eval_df = safe_load(eval_csv_path, kind=\"csv\")\n","if isinstance(eval_df, pd.DataFrame) and not eval_df.empty and \"Model\" in eval_df.columns:\n","    eval_df = eval_df[eval_df[\"Model\"].isin([\"LR\", \"GB\"])].copy()\n","    st.sidebar.success(\"✅ Evaluation summary loaded\")\n","    st.sidebar.dataframe(eval_df.head())\n","else:\n","    st.sidebar.warning(\"⚠️ Evaluation summary not found or empty\")\n","\n","# -------------------------\n","# Load models + thresholds\n","# -------------------------\n","models = {}\n","thresholds = {}\n","load_msgs = []\n","\n","for key in selected_models:\n","    mfile, tfile = expected_models[key]\n","    model = safe_load(os.path.join(MODELS_DIR, mfile), kind=\"pickle\")\n","    thr_obj = safe_load(os.path.join(MODELS_DIR, tfile), kind=\"pickle\")\n","    thr_val = threshold_controls.get(key, float(thr_obj) if thr_obj is not None else 0.5)\n","\n","    if model is not None:\n","        models[key] = model\n","        thresholds[key] = float(thr_val)\n","        load_msgs.append(f\"✅ {key} loaded (threshold={thr_val:.3f})\")\n","    else:\n","        load_msgs.append(f\"⚠️ {key} missing\")\n","\n","st.subheader(\"Model load status\")\n","st.write(\"\\n\".join(load_msgs))\n","\n","if not models:\n","    st.error(\"No models loaded. Please check your configuration.\")\n","    st.stop()\n","\n","# -------------------------\n","# Overview\n","# -------------------------\n","st.subheader(\"Overview\")\n","n_test = len(X_test)\n","models_list = \", \".join(models.keys())\n","\n","acc_range_txt = \"—\"\n","if isinstance(eval_df, pd.DataFrame) and \"Accuracy\" in eval_df.columns and not eval_df.empty:\n","    acc_min = eval_df[\"Accuracy\"].min()\n","    acc_max = eval_df[\"Accuracy\"].max()\n","    acc_range_txt = f\"{acc_min:.1f}%\" if acc_min == acc_max else f\"{acc_min:.1f}%–{acc_max:.1f}%\"\n","\n","c1, c2, c3 = st.columns(3)\n","with c1:\n","    st.metric(label=\"Test set size\", value=n_test)\n","with c2:\n","    st.metric(label=\"Models compared\", value=models_list)\n","with c3:\n","    st.metric(label=\"Accuracy range\", value=acc_range_txt)\n","\n","st.markdown(\n","    \"**Interpretability note:** LR provides interpretable coefficients (risk factors); \"\n","    \"GB provides feature importance and is evaluated with calibration curves for probability reliability.\"\n",")\n","\n","\n","\n","# -------------------------\n","# Model comparison table + bar plot\n","# -------------------------\n","st.subheader(\"Model comparison\")\n","if isinstance(eval_df, pd.DataFrame) and not eval_df.empty:\n","    cols_expected = [c for c in [\"Model\",\"Precision\",\"Recall\",\"F1\",\"Specificity\",\"ROC_AUC\",\"Accuracy\"] if c in eval_df.columns]\n","    if cols_expected:\n","        st.dataframe(eval_df[cols_expected], use_container_width=True)\n","        show_cols = [c for c in [\"Recall\",\"Specificity\",\"F1\",\"ROC_AUC\"] if c in eval_df.columns]\n","        if show_cols:\n","            fig_bar, ax = plt.subplots()\n","            eval_df.set_index(\"Model\")[show_cols].plot(\n","                kind=\"bar\", ax=ax,\n","                color=[\"#1f77b4\",\"#2ca02c\",\"#9467bd\",\"#ff7f0e\"]\n","            )\n","            ax.set_title(\"Balanced metrics comparison (Recall, Specificity, F1, ROC_AUC)\")\n","            ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n","            st.pyplot(fig_bar)\n","else:\n","    st.info(\"No evaluation_summary.csv found. Provide it to view the comparison table and charts.\")\n","\n","# -------------------------\n","# ROC curves overlay\n","# -------------------------\n","st.subheader(\"ROC curves\")\n","roc_curves = {}\n","color_map = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n","\n","for name, model in models.items():\n","    try:\n","        proba = model.predict_proba(X_test)[:, 1]\n","        auc = roc_auc_score(y_test, proba)\n","        fpr, tpr, _ = roc_curve(y_test, proba)\n","        roc_curves[name] = (fpr, tpr, auc, color_map.get(name, None))\n","    except Exception as e:\n","        st.warning(f\"Skipping ROC for {name}: {e}\")\n","\n","if roc_curves:\n","    fig_roc = plot_roc_curves(roc_curves, title=\"ROC curves by model (AUC in legend)\")\n","    st.pyplot(fig_roc)\n","\n","# -------------------------\n","# Calibration curves\n","# -------------------------\n","if show_calibration:\n","    st.subheader(\"Calibration curves\")\n","    cal_cols = st.columns(min(len(models), 3))\n","    for idx, (name, model) in enumerate(models.items()):\n","        try:\n","            if hasattr(model, \"predict_proba\"):\n","                y_prob = model.predict_proba(X_test)[:, 1]\n","                prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n","\n","                fig_cal, ax = plt.subplots()\n","                ax.plot(prob_pred, prob_true, marker='o', label=name, color=color_map.get(name, None))\n","                ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n","                ax.set_xlabel(\"Predicted probability\")\n","                ax.set_ylabel(\"True probability\")\n","                ax.set_title(f\"Calibration Curve - {name}\")\n","                ax.legend()\n","                with cal_cols[idx % len(cal_cols)]:\n","                    st.pyplot(fig_cal)\n","        except Exception as e:\n","            st.warning(f\"Skipping calibration for {name}: {e}\")\n","\n","# -------------------------\n","# Confusion matrices\n","# -------------------------\n","st.subheader(\"Confusion matrices\")\n","cm_cols = st.columns(min(len(models), 4))\n","for idx, (name, model) in enumerate(models.items()):\n","    thr = thresholds.get(name, 0.5)\n","    try:\n","        proba = model.predict_proba(X_test)[:, 1]\n","        y_pred = (proba >= thr).astype(int)\n","        fig_cm = plot_confusion_matrix(y_test, y_pred, title=f\"{name} (thr={thr:.3f})\")\n","        with cm_cols[idx % len(cm_cols)]:\n","            st.pyplot(fig_cm)\n","    except Exception as e:\n","        st.warning(f\"Skipping CM for {name}: {e}\")\n","\n","\n","\n","# -------------------------\n","# Model comparison table + bar plot\n","# -------------------------\n","st.subheader(\"Model comparison\")\n","if isinstance(eval_df, pd.DataFrame) and not eval_df.empty:\n","    cols_expected = [c for c in [\"Model\", \"Precision\", \"Recall\", \"F1\", \"Specificity\", \"ROC_AUC\", \"Accuracy\"] if c in eval_df.columns]\n","    if cols_expected:\n","        st.dataframe(eval_df[cols_expected], use_container_width=True)\n","\n","        show_cols = [c for c in [\"Recall\", \"Specificity\", \"F1\", \"ROC_AUC\"] if c in eval_df.columns]\n","        if show_cols:\n","            fig_bar, ax = plt.subplots()\n","            eval_df.set_index(\"Model\")[show_cols].plot(\n","                kind=\"bar\", ax=ax,\n","                color=[\"#1f77b4\", \"#2ca02c\", \"#9467bd\", \"#ff7f0e\"]\n","            )\n","            ax.set_title(\"Balanced metrics comparison (Recall, Specificity, F1, ROC_AUC)\")\n","            ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n","            st.pyplot(fig_bar)\n","else:\n","    st.info(\"No evaluation_summary.csv found. Provide it to view the comparison table and charts.\")\n","\n","# -------------------------\n","# ROC curves overlay\n","# -------------------------\n","st.subheader(\"ROC curves\")\n","roc_curves = {}\n","color_map = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n","\n","for name, model in models.items():\n","    try:\n","        proba = model.predict_proba(X_test)[:, 1]\n","        auc = roc_auc_score(y_test, proba)\n","        fpr, tpr, _ = roc_curve(y_test, proba)\n","        roc_curves[name] = (fpr, tpr, auc, color_map.get(name, None))\n","    except Exception as e:\n","        st.warning(f\"Skipping ROC for {name}: {e}\")\n","\n","if roc_curves:\n","    fig_roc = plot_roc_curves(roc_curves, title=\"ROC curves by model (AUC in legend)\")\n","    st.pyplot(fig_roc)\n","\n","# -------------------------\n","# Calibration curves\n","# -------------------------\n","if show_calibration:\n","    st.subheader(\"Calibration curves\")\n","    cal_cols = st.columns(min(len(models), 3))\n","    for idx, (name, model) in enumerate(models.items()):\n","        try:\n","            if hasattr(model, \"predict_proba\"):\n","                y_prob = model.predict_proba(X_test)[:, 1]\n","                prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n","\n","                fig_cal, ax = plt.subplots()\n","                ax.plot(prob_pred, prob_true, marker='o', label=name, color=color_map.get(name, None))\n","                ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n","                ax.set_xlabel(\"Predicted probability\")\n","                ax.set_ylabel(\"True probability\")\n","                ax.set_title(f\"Calibration Curve - {name}\")\n","                ax.legend()\n","                with cal_cols[idx % len(cal_cols)]:\n","                    st.pyplot(fig_cal)\n","        except Exception as e:\n","            st.warning(f\"Skipping calibration for {name}: {e}\")\n","\n","# -------------------------\n","# Confusion matrices\n","# -------------------------\n","st.subheader(\"Confusion matrices\")\n","cm_cols = st.columns(min(len(models), 4))\n","for idx, (name, model) in enumerate(models.items()):\n","    thr = thresholds.get(name, 0.5)\n","    try:\n","        proba = model.predict_proba(X_test)[:, 1]\n","        y_pred = (proba >= thr).astype(int)\n","        fig_cm = plot_confusion_matrix(y_test, y_pred, title=f\"{name} (thr={thr:.3f})\")\n","        with cm_cols[idx % len(cm_cols)]:\n","            st.pyplot(fig_cm)\n","    except Exception as e:\n","        st.warning(f\"Skipping CM for {name}: {e}\")\n","\n","\n","\n","# -------------------------\n","# Interactive prediction (single case) with clinical reliability metrics\n","# -------------------------\n","st.subheader(\"Interactive prediction\")\n","feature_cols = list(X_test.columns)\n","numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(X_test[c])]\n","use_cols = numeric_cols[:6] if len(numeric_cols) >= 2 else feature_cols\n","\n","with st.form(\"single_prediction\"):\n","    inputs = {}\n","    for c in use_cols:\n","        default_val = float(np.nanmean(X_test[c])) if c in X_test.columns else 0.0\n","        inputs[c] = st.number_input(c, value=default_val)\n","    model_choice = st.selectbox(\"Model\", options=list(models.keys()), key=\"single_model\")\n","    submitted = st.form_submit_button(\"Predict\")\n","\n","if submitted:\n","    try:\n","        df_in = pd.DataFrame([inputs])\n","        model = models[model_choice]\n","        thr = thresholds.get(model_choice, 0.5)\n","        proba = model.predict_proba(df_in)[:, 1][0]\n","        pred = int(proba >= thr)\n","\n","        # Reliability metrics from test set\n","        y_prob_test = model.predict_proba(X_test)[:, 1]\n","        y_pred_test = (y_prob_test >= thr).astype(int)\n","        ppv = compute_ppv(y_test, y_pred_test)\n","        npv = compute_npv(y_test, y_pred_test)\n","        brier = compute_brier(y_test, y_prob_test)\n","\n","        st.success(f\"Probability: {proba:.3f} | Prediction: {'Malignant' if pred==1 else 'Benign'} (thr={thr:.3f})\")\n","        st.info(f\"PPV: {ppv:.3f} | NPV: {npv:.3f} | Brier score: {brier:.3f}\")\n","    except Exception as e:\n","        st.error(f\"Prediction failed: {e}\")\n","\n","# -------------------------\n","# Decision Curve Analysis\n","# -------------------------\n","st.header(\"Decision Curve Analysis\")\n","\n","dca_colors = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n","dca_frames = []\n","fig_dca, ax = plt.subplots()\n","\n","for name, model in models.items():\n","    try:\n","        y_prob = model.predict_proba(X_test)[:, 1]\n","        dca_df = decision_curve(y_test, y_prob)\n","        dca_df[\"Model\"] = name\n","        dca_frames.append(dca_df)\n","        ax.plot(dca_df[\"threshold\"], dca_df[\"net_benefit\"], label=name,\n","                color=dca_colors.get(name, None), linewidth=2)\n","    except Exception as e:\n","        st.warning(f\"Skipping DCA for {name}: {e}\")\n","\n","# Baseline strategies\n","prevalence = y_test.mean()\n","thr_grid = np.linspace(0.01, 0.99, 50)\n","treat_all = [prevalence - (1 - prevalence) * (thr / (1 - thr)) for thr in thr_grid]\n","treat_none = [0 for _ in thr_grid]\n","ax.plot(thr_grid, treat_all, linestyle=\"--\", color=\"black\", label=\"Treat All\")\n","ax.plot(thr_grid, treat_none, linestyle=\":\", color=\"gray\", label=\"Treat None\")\n","\n","ax.set_xlabel(\"Threshold probability\")\n","ax.set_ylabel(\"Net benefit\")\n","ax.set_title(\"Decision Curve Analysis\")\n","ax.legend(loc=\"best\")\n","st.pyplot(fig_dca)\n","\n","# -------------------------\n","# Export DCA results\n","# -------------------------\n","if dca_frames:\n","    dca_all = pd.concat(dca_frames, ignore_index=True)\n","    st.subheader(\"Decision Curve Data\")\n","    st.dataframe(dca_all.head(20), use_container_width=True)\n","\n","    # Offer download of combined DCA results\n","    csv_data = dca_all.to_csv(index=False).encode(\"utf-8\")\n","    st.download_button(\n","        label=\"Download DCA results (CSV)\",\n","        data=csv_data,\n","        file_name=\"decision_curve.csv\",\n","        mime=\"text/csv\"\n","    )\n","else:\n","    st.info(\"No DCA results available.\")\n","\n","# -------------------------\n","# Artifact handling\n","# -------------------------\n","st.subheader(\"Artifacts\")\n","st.info(\"Figures and results are displayed directly in the dashboard. Use the download buttons above to export data.\")"]},{"cell_type":"markdown","metadata":{"id":"dhQAUs-Udfoc"},"source":["# Deployment Guide\n","\n","This section explains how to reuse models outside the dashboard and apply them to new patient data.\n","\n","\n","\n","## 1. Export Model + Threshold Bundle\n","- **Purpose:** Save a trained model with its tuned threshold and feature list for reuse.  \n","- **Contents of the bundle:**\n","  - Model path (location of the serialized model file)\n","  - Model type (e.g., Logistic Regression, Random Forest)\n","  - Tuned threshold value\n","  - Feature list (columns expected in input data)\n","- **Usage:** Exported bundles can be loaded in other environments (e.g., Colab, Streamlit apps) to ensure consistent predictions.\n","\n","\n","\n","## 2. Batch Scoring\n","- **Purpose:** Score multiple new patient records at once.  \n","- **Steps:**\n","  1. Upload a CSV file containing patient features.  \n","  2. Select the model to use for scoring.  \n","  3. The dashboard outputs:\n","     - Predicted probability of malignancy\n","     - Final prediction (Benign/Malignant) based on the tuned threshold\n","  4. Results are saved as a CSV in the artifacts directory for reproducibility.\n","- **Note:** Uploaded CSV must align with the training feature set. Missing columns are automatically filled with default values.\n","\n","\n","\n","## 3. Interactive Prediction (Single Case)\n","- **Purpose:** Test the model on a single patient case interactively.  \n","- **Steps:**\n","  1. Enter values for selected features (defaults are pre-filled with dataset averages).  \n","  2. Choose a model and click **Predict**.  \n","  3. The dashboard displays:\n","     - Probability of malignancy\n","     - Final prediction (Benign/Malignant)\n","- **Optional:** If SHAP is enabled, a local explanation plot shows which features most influenced the prediction.\n","\n","\n","\n","## 4. Artifacts Management\n","- **Purpose:** Ensure reproducibility and easy sharing of results.  \n","- **Artifacts saved include:**\n","  - ROC curves\n","  - Confusion matrices\n","  - Calibration curves\n","  - SHAP plots\n","  - Threshold tuning metrics CSV\n","  - Batch scoring results\n","- **Download options:**\n","  - Individual files (plots, CSVs, models)\n","  - Full ZIP bundle containing all artifacts\n","  \n","  \n","\n"," **Key Takeaway:**  \n","This deployment workflow ensures that models trained in the dashboard can be exported, reused, and explained consistently. Batch scoring supports large datasets, while interactive prediction enables case-by-case interpretability."]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbM9DOiEesMi6iwLRoIv9z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}