{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx3WX9BoAg3h"
   },
   "source": [
    "# Final Report & Visual Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-_GCZw9AjNQ"
   },
   "source": [
    "## Introduction\n",
    "This notebook distills our modelling journey into a visual and interpretive dashboard. We highlight performance, feature importance, and dimensionality insights- bridging technical rigor with narrative clarity.\n",
    "\n",
    "**Goal:** Create an interactive dashboard to explore model predictions, feature importance, and class separation. This notebook wraps up your workflow with visual storytelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUYMG_8LLWoO"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765578977222,
     "user": {
      "displayName": "Yasmine Ali-Osman",
      "userId": "01601773200894379900"
     },
     "user_tz": 0
    },
    "id": "qWLDE6Uwd3HE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Thresholds saved to models/\n",
      "âœ… Exported X_test.csv and y_test.csv to dashboard/\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Directories as raw strings\n",
    "MODELS_DIR = r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models\"\n",
    "DASHBOARD_DIR = r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard\"\n",
    "\n",
    "# Load test data directly from .pkl\n",
    "X_test = joblib.load(r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models/X_test.pkl\")\n",
    "y_test = joblib.load(r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models/y_test.pkl\")\n",
    "\n",
    "# Define thresholds\n",
    "threshold_lr = 0.45\n",
    "threshold_gb = 0.39\n",
    "\n",
    "# Save thresholds directly\n",
    "joblib.dump(threshold_lr, r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models/threshold_lr.pkl\")\n",
    "joblib.dump(threshold_gb, r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models/threshold_gb.pkl\")\n",
    "\n",
    "print(\"âœ… Thresholds saved to models/\")\n",
    "\n",
    "# Export test data to CSV directly\n",
    "pd.DataFrame(X_test).to_csv(r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard/X_test.csv\", index=False)\n",
    "pd.DataFrame({\"label\": y_test}).to_csv(r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard/y_test.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Exported X_test.csv and y_test.csv to dashboard/\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1765578975545,
     "user": {
      "displayName": "Yasmine Ali-Osman",
      "userId": "01601773200894379900"
     },
     "user_tz": 0
    },
    "id": "hsgjeHU8RHQw",
    "outputId": "b4a41b52-22c3-4366-d278-38cd83cf4570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting breast_cancer_06_dashboard.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile breast_cancer_06_dashboard.py\n",
    "\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# -------------------------\n",
    "# App config\n",
    "# -------------------------\n",
    "st.set_page_config(page_title=\"Breast Cancer Prediction Dashboard\", layout=\"wide\")\n",
    "st.title(\"Breast Cancer Prediction Dashboard\")\n",
    "st.caption(\"This dashboard is for research/decision support, not a substitute for medical diagnosis.\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths (raw strings only)\n",
    "# -------------------------\n",
    "MODELS_DIR = r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/models\"\n",
    "DASHBOARD_DIR = r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard\"\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def specificity_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp = cm[0,0], cm[0,1]\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "def compute_ppv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp, fp = cm[1,1], cm[0,1]\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "\n",
    "def compute_npv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fn = cm[0,0], cm[1,0]\n",
    "    return tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "\n",
    "def compute_brier(y_true, y_prob):\n",
    "    return brier_score_loss(y_true, y_prob)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels([\"Benign\", \"Malignant\"])\n",
    "    ax.set_yticklabels([\"Benign\", \"Malignant\"], rotation=0)\n",
    "    return fig\n",
    "\n",
    "def plot_roc_curves(curves, title=\"ROC curves\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0,1],[0,1], \"k--\", label=\"Chance\")\n",
    "    for name, (fpr, tpr, auc, color) in curves.items():\n",
    "        ax.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\", color=color)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return fig\n",
    "\n",
    "def decision_curve(y_true, y_prob, thresholds=np.linspace(0.01, 0.99, 50)):\n",
    "    \"\"\"\n",
    "    Net benefit = (TP/n) - (FP/n) * (threshold / (1 - threshold))\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        net_benefit = (tp/n) - (fp/n) * (thr / (1 - thr))\n",
    "        rows.append({\"threshold\": thr, \"net_benefit\": net_benefit})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Sidebar\n",
    "# -------------------------\n",
    "st.sidebar.header(\"Configuration\")\n",
    "\n",
    "# Keep only test data paths\n",
    "x_test_path = st.sidebar.text_input(\n",
    "    \"X_test CSV (features)\",\n",
    "    r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard/X_test.csv\"\n",
    ")\n",
    "y_test_path = st.sidebar.text_input(\n",
    "    \"y_test CSV (labels)\",\n",
    "    r\"C:/Users/yasmine/Documents/Portfolio/DataSciencePortfolio/Projects/Breast-Cancer/dashboard/y_test.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model selection\n",
    "selected_models = st.sidebar.multiselect(\n",
    "    \"Models to include\",\n",
    "    options=[\"LR\", \"GB\"],\n",
    "    default=[\"LR\", \"GB\"]\n",
    ")\n",
    "\n",
    "# Threshold tuning sliders\n",
    "st.sidebar.subheader(\"Threshold tuning\")\n",
    "threshold_controls = {}\n",
    "for m in selected_models:\n",
    "    # Set default slider value per model\n",
    "    default_val = 0.5\n",
    "    if m == \"LR\":\n",
    "        default_val = 0.45\n",
    "    elif m == \"GB\":\n",
    "        default_val = 0.39\n",
    "\n",
    "    threshold_controls[m] = st.sidebar.slider(\n",
    "        f\"{m} threshold\",\n",
    "        0.0, 1.0, default_val, 0.01\n",
    "    )\n",
    "\n",
    "# Only keep calibration checkbox\n",
    "show_calibration = st.sidebar.checkbox(\"Show calibration curves\", value=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load test data\n",
    "# -------------------------\n",
    "try:\n",
    "    X_test = pd.read_csv(x_test_path)\n",
    "    y_test_df = pd.read_csv(y_test_path)\n",
    "    y_test = y_test_df.iloc[:, 0]\n",
    "    if X_test.empty or y_test.empty:\n",
    "        st.error(\"âŒ One or both test files are empty. Please check X_test.csv and y_test.csv.\")\n",
    "        st.stop()\n",
    "    st.success(f\"âœ… Test data loaded (X_test shape: {X_test.shape}, y_test length: {len(y_test)})\")\n",
    "except Exception as e:\n",
    "    st.error(f\"âŒ Failed to load test data: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load models + thresholds\n",
    "# -------------------------\n",
    "models = {}\n",
    "thresholds = {}\n",
    "load_msgs = []\n",
    "\n",
    "for key in selected_models:\n",
    "    # Model\n",
    "    try:\n",
    "        model_path = rf\"{MODELS_DIR}/model_{key.lower()}.pkl\"\n",
    "        model = joblib.load(model_path)\n",
    "        models[key] = model\n",
    "        # Threshold\n",
    "        try:\n",
    "            thr_path = rf\"{MODELS_DIR}/threshold_{key.lower()}.pkl\"\n",
    "            thr = joblib.load(thr_path)\n",
    "        except Exception:\n",
    "            thr = threshold_controls.get(key, 0.5)\n",
    "            st.sidebar.warning(f\"âš ï¸ Threshold file missing for {key}. Using slider/default = {thr:.2f}\")\n",
    "        thresholds[key] = float(thr)\n",
    "        load_msgs.append(f\"âœ… {key} loaded (threshold={thresholds[key]:.3f})\")\n",
    "    except Exception as e:\n",
    "        load_msgs.append(f\"âš ï¸ {key} model file missing or unreadable: {e}\")\n",
    "\n",
    "st.subheader(\"Model load status\")\n",
    "for msg in load_msgs:\n",
    "    st.write(msg)\n",
    "if not models:\n",
    "    st.error(\"âŒ No models loaded. Please ensure .pkl files exist in models/.\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Metric helper functions\n",
    "# -------------------------\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, brier_score_loss\n",
    "\n",
    "def _specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp = cm[0,0], cm[0,1]\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "def _ppv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp, fp = cm[1,1], cm[0,1]\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "\n",
    "def _npv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fn = cm[0,0], cm[1,0]\n",
    "    return tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "\n",
    "def _compute_threshold_metrics(name, model, X, y, thr):\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Threshold\": thr,\n",
    "        \"Precision\": precision_score(y, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y, y_pred, zero_division=0),\n",
    "        \"Specificity\": _specificity(y, y_pred),\n",
    "        \"Accuracy\": accuracy_score(y, y_pred),\n",
    "        \"ROC_AUC\": roc_auc_score(y, y_prob),\n",
    "        \"PPV\": _ppv(y, y_pred),\n",
    "        \"NPV\": _npv(y, y_pred),\n",
    "        \"Brier\": brier_score_loss(y, y_prob)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Overview\n",
    "# -------------------------\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "st.subheader(\"Overview\")\n",
    "\n",
    "\n",
    "# Test set size\n",
    "n_test = len(X_test)\n",
    "\n",
    "# Models compared\n",
    "models_list = \", \".join(models.keys()) if models else \"â€”\"\n",
    "\n",
    "# Compute accuracy range directly from models\n",
    "acc_values = []\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        acc_values.append(acc * 100)\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Could not compute accuracy for {name}: {e}\")\n",
    "\n",
    "acc_range_txt = \"â€”\"\n",
    "if acc_values:\n",
    "    acc_min = min(acc_values)\n",
    "    acc_max = max(acc_values)\n",
    "    acc_range_txt = f\"{acc_min:.1f}%\" if acc_min == acc_max else f\"{acc_min:.1f}%â€“{acc_max:.1f}%\"\n",
    "\n",
    "# Display metrics\n",
    "c1, c2, c3 = st.columns(3)\n",
    "with c1:\n",
    "    st.metric(label=\"Test set size\", value=n_test)\n",
    "with c2:\n",
    "    st.metric(label=\"Models compared\", value=models_list)\n",
    "with c3:\n",
    "    st.metric(label=\"Accuracy range\", value=acc_range_txt)\n",
    "\n",
    "# Interpretability note\n",
    "st.markdown(\"\"\"\n",
    "**Interpretation Notes:**\n",
    "- The very high accuracy range (99â€“100%) may reflect:\n",
    "  - **Feature separability:** During exploratory analysis (EDA), PCA showed clear clustering between benign and malignant cases, suggesting the dataset is inherently well-separated.\n",
    "  - **Class balance handling:** We used stratified train/test splits to preserve class ratios, reducing bias from imbalance.\n",
    "  - **Test size:** The hold-out test set is relatively small (114 cases), which can inflate performance metrics.\n",
    "- **ROC curves** confirm separability: both models achieve near-perfect AUC, meaning they can distinguish malignant from benign cases almost flawlessly on this dataset.\n",
    "- **Calibration curves** demonstrate probability reliability: predicted risks align with observed outcomes, which is essential for clinical trust.\n",
    "- **Decision curve analysis (DCA)** shows positive net benefit compared to treating all or none, providing evidence of clinical utility.\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------\n",
    "# Model comparison table + bar plot (live metrics)\n",
    "# -------------------------\n",
    "st.subheader(\"Model comparison\")\n",
    "\n",
    "metrics_rows = []\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            thr = thresholds.get(name, 0.5)\n",
    "            metrics_rows.append(_compute_threshold_metrics(name, model, X_test, y_test, thr))\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Skipping metrics for {name}: {e}\")\n",
    "\n",
    "if metrics_rows:\n",
    "    eval_df = pd.DataFrame(metrics_rows, columns=[\n",
    "        \"Model\", \"Threshold\", \"Precision\", \"Recall\", \"F1\", \"Specificity\",\n",
    "        \"Accuracy\", \"ROC_AUC\", \"PPV\", \"NPV\", \"Brier\"\n",
    "    ])\n",
    "    st.dataframe(eval_df[[\"Model\",\"Precision\",\"Recall\",\"F1\",\"Specificity\",\"ROC_AUC\",\"Accuracy\"]],\n",
    "                 use_container_width=True)\n",
    "\n",
    "    show_cols = [\"Recall\",\"Specificity\",\"F1\",\"ROC_AUC\"]\n",
    "    fig_bar, ax = plt.subplots()\n",
    "    eval_df.set_index(\"Model\")[show_cols].plot(\n",
    "        kind=\"bar\", ax=ax,\n",
    "        color=[\"#1f77b4\", \"#2ca02c\", \"#9467bd\", \"#ff7f0e\"]\n",
    "    )\n",
    "    ax.set_title(\"Balanced metrics comparison (Recall, Specificity, F1, ROC_AUC)\")\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    st.pyplot(fig_bar)\n",
    "else:\n",
    "    st.info(\"No live metrics available. Ensure models are loaded and support predict_proba.\")\n",
    "\n",
    "# -------------------------\n",
    "# ROC curves overlay (live)\n",
    "# -------------------------\n",
    "st.subheader(\"ROC curves\")\n",
    "roc_curves = {}\n",
    "color_map = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, proba)\n",
    "            fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "            roc_curves[name] = (fpr, tpr, auc, color_map.get(name, None))\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Skipping ROC for {name}: {e}\")\n",
    "\n",
    "if roc_curves:\n",
    "    fig_roc, ax = plt.subplots()\n",
    "    ax.plot([0,1],[0,1],\"k--\",label=\"Chance\")\n",
    "    for name,(fpr,tpr,auc,color) in roc_curves.items():\n",
    "        ax.plot(fpr,tpr,label=f\"{name} (AUC={auc:.3f})\",color=color)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC curves by model (AUC in legend)\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    st.pyplot(fig_roc)\n",
    "else:\n",
    "    st.info(\"No ROC curves available. Ensure models are loaded and support predict_proba.\")\n",
    "\n",
    "# -------------------------\n",
    "# Calibration curves (live)\n",
    "# -------------------------\n",
    "if show_calibration and models:\n",
    "    st.subheader(\"Calibration curves\")\n",
    "    cal_cols = st.columns(min(len(models), 3))\n",
    "    for idx,(name,model) in enumerate(models.items()):\n",
    "        try:\n",
    "            if hasattr(model,\"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_test)[:,1]\n",
    "                prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "\n",
    "                fig_cal, ax = plt.subplots()\n",
    "                ax.plot(prob_pred, prob_true, marker='o', label=name, color=color_map.get(name,None))\n",
    "                ax.plot([0,1],[0,1],\"k--\",label=\"Perfectly calibrated\")\n",
    "                ax.set_xlabel(\"Predicted probability\")\n",
    "                ax.set_ylabel(\"True probability\")\n",
    "                ax.set_title(f\"Calibration Curve - {name}\")\n",
    "                ax.legend()\n",
    "                with cal_cols[idx % len(cal_cols)]:\n",
    "                    st.pyplot(fig_cal)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Skipping calibration for {name}: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrices (live)\n",
    "# -------------------------\n",
    "st.subheader(\"Confusion matrices\")\n",
    "if models:\n",
    "    cm_cols = st.columns(min(len(models), 4))\n",
    "    for idx,(name,model) in enumerate(models.items()):\n",
    "        thr = thresholds.get(name,0.5)\n",
    "        try:\n",
    "            if hasattr(model,\"predict_proba\"):\n",
    "                proba = model.predict_proba(X_test)[:,1]\n",
    "                y_pred = (proba >= thr).astype(int)\n",
    "                fig_cm = plot_confusion_matrix(y_test, y_pred, title=f\"{name} (thr={thr:.3f})\")\n",
    "                with cm_cols[idx % len(cm_cols)]:\n",
    "                    st.pyplot(fig_cm)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Skipping CM for {name}: {e}\")\n",
    "else:\n",
    "    st.info(\"No models available to generate confusion matrices.\")\n",
    "\n",
    "# -------------------------\n",
    "# Interpretability (live generation)\n",
    "# -------------------------\n",
    "st.header(\"Interpretability\")\n",
    "\n",
    "def _specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp = cm[0,0], cm[0,1]\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "def _ppv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp, fp = cm[1,1], cm[0,1]\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "\n",
    "def _npv(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fn = cm[0,0], cm[1,0]\n",
    "    return tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "\n",
    "def _compute_threshold_metrics(name, model, X, y, thr):\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Threshold\": thr,\n",
    "        \"Precision\": precision_score(y, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y, y_pred, zero_division=0),\n",
    "        \"Specificity\": _specificity(y, y_pred),\n",
    "        \"Accuracy\": accuracy_score(y, y_pred),\n",
    "        \"ROC_AUC\": roc_auc_score(y, y_prob),\n",
    "        \"PPV\": _ppv(y, y_pred),\n",
    "        \"NPV\": _npv(y, y_pred),\n",
    "        \"Brier\": brier_score_loss(y, y_prob)\n",
    "    }\n",
    "\n",
    "st.subheader(\"Threshold tuning metrics\")\n",
    "metrics_rows = []\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            thr = thresholds.get(name, 0.5)\n",
    "            metrics_rows.append(_compute_threshold_metrics(name, model, X_test, y_test, thr))\n",
    "        else:\n",
    "            st.warning(f\"{name} does not support predict_proba; skipping metrics.\")\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Could not compute metrics for {name}: {e}\")\n",
    "\n",
    "if metrics_rows:\n",
    "    thr_metrics_df = pd.DataFrame(metrics_rows, columns=[\n",
    "        \"Model\", \"Threshold\", \"Precision\", \"Recall\", \"F1\", \"Specificity\",\n",
    "        \"Accuracy\", \"ROC_AUC\", \"PPV\", \"NPV\", \"Brier\"\n",
    "    ])\n",
    "    st.dataframe(thr_metrics_df, use_container_width=True)\n",
    "    csv_data = thr_metrics_df.to_csv(index=False).encode(\"utf-8\")\n",
    "    st.download_button(\n",
    "        label=\"Download threshold tuning metrics (CSV)\",\n",
    "        data=csv_data,\n",
    "        file_name=\"threshold_tuning_metrics.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "else:\n",
    "    st.info(\"No threshold metrics available. Ensure models are loaded and support predict_proba.\")\n",
    "\n",
    "\n",
    "\n",
    "st.subheader(\"Feature importance and risk factors\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        # ðŸ”‘ Unwrap pipeline if needed\n",
    "        if hasattr(model, \"named_steps\") and \"clf\" in model.named_steps:\n",
    "            base_model = model.named_steps[\"clf\"]\n",
    "        else:\n",
    "            base_model = model\n",
    "\n",
    "        # Logistic Regression coefficients\n",
    "        if name == \"LR\" and hasattr(base_model, \"coef_\"):\n",
    "            coefs = base_model.coef_[0]\n",
    "            feat_imp_lr = pd.DataFrame({\n",
    "                \"Feature\": X_test.columns,\n",
    "                \"Coefficient\": coefs,\n",
    "                \"Impact\": [\"â†‘ risk\" if c > 0 else \"â†“ risk\" for c in coefs]\n",
    "            }).sort_values(\"Coefficient\", ascending=False)\n",
    "\n",
    "            fig_lr, ax = plt.subplots(figsize=(8, max(4, len(feat_imp_lr) * 0.25)))\n",
    "            sns.barplot(\n",
    "                x=\"Coefficient\", y=\"Feature\", hue=\"Impact\", data=feat_imp_lr,\n",
    "                palette={\"â†‘ risk\": \"darkred\", \"â†“ risk\": \"steelblue\"}, dodge=False, ax=ax\n",
    "            )\n",
    "            ax.set_title(\"LR feature impacts (Positive = higher malignancy risk)\")\n",
    "            ax.legend(title=\"Impact\", loc=\"best\")\n",
    "            st.pyplot(fig_lr)\n",
    "            st.dataframe(feat_imp_lr)\n",
    "\n",
    "        # Gradient Boosting feature importances\n",
    "        elif name == \"GB\" and hasattr(base_model, \"feature_importances_\"):\n",
    "            importances = base_model.feature_importances_\n",
    "            feat_imp_gb = pd.DataFrame({\n",
    "                \"Feature\": X_test.columns,\n",
    "                \"Importance\": importances\n",
    "            }).sort_values(\"Importance\", ascending=False).head(15)\n",
    "\n",
    "            fig_gb, ax = plt.subplots(figsize=(8, max(4, len(feat_imp_gb) * 0.25)))\n",
    "            sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp_gb,\n",
    "                        palette=\"viridis\", dodge=False, ax=ax)\n",
    "            ax.set_title(\"GB feature importance (top 15)\")\n",
    "            st.pyplot(fig_gb)\n",
    "            st.dataframe(feat_imp_gb)\n",
    "\n",
    "        else:\n",
    "            st.warning(f\"{name} does not expose coefficients or feature importances.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Skipping interpretability for {name}: {e}\")\n",
    "\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "**Interpretability notes:**\n",
    "- Logistic Regression (LR) coefficients act as risk factors: positive values increase malignancy risk, negative values decrease it.\n",
    "- Gradient Boosting (GB) provides relative feature importance scores; calibration curves assess probability reliability.\n",
    "- Threshold tuning metrics (Precision, Recall, Specificity, PPV, NPV, Brier) are generated live from the test set at your selected thresholds.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Interactive prediction\n",
    "# -------------------------\n",
    "st.subheader(\"Interactive prediction\")\n",
    "\n",
    "if X_test is not None and not X_test.empty and models:\n",
    "    feature_cols = list(X_test.columns)\n",
    "    numeric_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(X_test[c])]\n",
    "\n",
    "    # Option to use all or top 15 features\n",
    "    mode_choice = st.radio(\"Select input mode\", [\"All features\", \"Top 15 features\"], index=0)\n",
    "\n",
    "    if mode_choice == \"Top 15 features\":\n",
    "        # For simplicity, use variance or importance ranking if available\n",
    "        # Here we just take first 15 numeric columns\n",
    "        use_cols = numeric_cols[:15] if len(numeric_cols) >= 15 else numeric_cols\n",
    "    else:\n",
    "        use_cols = numeric_cols\n",
    "\n",
    "    with st.form(\"single_prediction\"):\n",
    "        inputs = {}\n",
    "        for c in use_cols:\n",
    "            try:\n",
    "                default_val = float(np.nanmean(X_test[c])) if c in X_test.columns else 0.0\n",
    "            except Exception:\n",
    "                default_val = 0.0\n",
    "            inputs[c] = st.number_input(c, value=default_val)\n",
    "        model_choice = st.selectbox(\"Model\", options=list(models.keys()), key=\"single_model\")\n",
    "        submitted = st.form_submit_button(\"Predict\")\n",
    "\n",
    "    if submitted:\n",
    "        try:\n",
    "            # Build full input row with all features\n",
    "            df_in = pd.DataFrame([np.zeros(len(feature_cols))], columns=feature_cols)\n",
    "            for feat, val in inputs.items():\n",
    "                if feat in df_in.columns:\n",
    "                    df_in.at[0, feat] = val\n",
    "\n",
    "            model = models[model_choice]\n",
    "            thr = thresholds.get(model_choice, 0.5)\n",
    "\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                proba = model.predict_proba(df_in)[:, 1][0]\n",
    "                pred = int(proba >= thr)\n",
    "\n",
    "                y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred_test = (y_prob_test >= thr).astype(int)\n",
    "                ppv = _ppv(y_test, y_pred_test)\n",
    "                npv = _npv(y_test, y_pred_test)\n",
    "                brier = brier_score_loss(y_test, y_prob_test)\n",
    "\n",
    "                st.success(f\"Probability: {proba:.3f} | Prediction: {'Malignant' if pred==1 else 'Benign'} (thr={thr:.3f})\")\n",
    "                st.info(f\"PPV: {ppv:.3f} | NPV: {npv:.3f} | Brier score: {brier:.3f}\")\n",
    "            else:\n",
    "                st.error(f\"Selected model {model_choice} does not support probability predictions.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Prediction failed: {e}\")\n",
    "else:\n",
    "    st.info(\"Interactive prediction unavailable. Ensure test data and models are loaded.\")\n",
    "\n",
    "# -------------------------\n",
    "# Decision Curve Analysis\n",
    "# -------------------------\n",
    "st.header(\"Decision Curve Analysis\")\n",
    "\n",
    "dca_colors = {\"LR\": \"#1f77b4\", \"GB\": \"#ff7f0e\"}\n",
    "dca_frames = []\n",
    "fig_dca, ax = plt.subplots()\n",
    "\n",
    "if models and X_test is not None and y_test is not None:\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_test)[:, 1]\n",
    "                # Net benefit\n",
    "                n = len(y_test)\n",
    "                thr_grid = np.linspace(0.01, 0.99, 50)\n",
    "                rows = []\n",
    "                for thr in thr_grid:\n",
    "                    y_pred = (y_prob >= thr).astype(int)\n",
    "                    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "                    net_benefit = (tp/n) - (fp/n) * (thr / (1 - thr))\n",
    "                    rows.append({\"threshold\": thr, \"net_benefit\": net_benefit})\n",
    "                dca_df = pd.DataFrame(rows)\n",
    "                dca_df[\"Model\"] = name\n",
    "                dca_frames.append(dca_df)\n",
    "\n",
    "                ax.plot(dca_df[\"threshold\"], dca_df[\"net_benefit\"], label=name,\n",
    "                        color=dca_colors.get(name, None), linewidth=2)\n",
    "        except Exception as e:\n",
    "            st.warning(f\"Skipping DCA for {name}: {e}\")\n",
    "\n",
    "    # Baselines\n",
    "    prevalence = y_test.mean()\n",
    "    thr_grid = np.linspace(0.01, 0.99, 50)\n",
    "    treat_all = [prevalence - (1 - prevalence) * (thr / (1 - thr)) for thr in thr_grid]\n",
    "    treat_none = [0 for _ in thr_grid]\n",
    "    ax.plot(thr_grid, treat_all, linestyle=\"--\", color=\"black\", label=\"Treat All\")\n",
    "    ax.plot(thr_grid, treat_none, linestyle=\":\", color=\"gray\", label=\"Treat None\")\n",
    "\n",
    "    ax.set_xlabel(\"Threshold probability\")\n",
    "    ax.set_ylabel(\"Net benefit\")\n",
    "    ax.set_title(\"Decision Curve Analysis\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    st.pyplot(fig_dca)\n",
    "else:\n",
    "    st.info(\"Decision Curve Analysis unavailable. Ensure test data and models are loaded.\")    \n",
    "\n",
    "# -------------------------\n",
    "# Export DCA results\n",
    "# -------------------------\n",
    "if dca_frames:\n",
    "    dca_all = pd.concat(dca_frames, ignore_index=True)\n",
    "    st.subheader(\"Decision Curve Data\")\n",
    "    st.dataframe(dca_all.head(20), use_container_width=True)\n",
    "    csv_data = dca_all.to_csv(index=False).encode(\"utf-8\")\n",
    "    st.download_button(\n",
    "        label=\"Download DCA results (CSV)\",\n",
    "        data=csv_data,\n",
    "        file_name=\"decision_curve.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "else:\n",
    "    st.info(\"No DCA results available.\")\n",
    "\n",
    "\n",
    " # -------------------------\n",
    "# Upload Your Own Dataset\n",
    "# -------------------------\n",
    "st.markdown(\"\"\"\n",
    "### Upload Your Own Dataset\n",
    "You can upload a CSV file with the same feature structure used during training.  \n",
    "- The file should include all required feature columns (e.g., tumor measurements).  \n",
    "- If a `diagnosis` column is present, it will be used as the label for evaluation.  \n",
    "- If no label column is present, the dashboard will still generate predictions but not evaluation metrics.  \n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    expected_features = joblib.load(f\"{file_path}/feature_names.pkl\")\n",
    "except Exception:\n",
    "    expected_features = list(X_test.columns)\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a dataset (CSV)\", type=[\"csv\"])\n",
    "if uploaded_file is not None:\n",
    "    new_data = pd.read_csv(uploaded_file)\n",
    "    st.write(\"Uploaded dataset shape:\", new_data.shape)\n",
    "\n",
    "    # Check feature compatibility\n",
    "    missing_features = [f for f in expected_features if f not in new_data.columns]\n",
    "    if missing_features:\n",
    "        st.error(f\"Dataset is missing required features: {missing_features}\")\n",
    "    else:\n",
    "        st.success(\"Dataset matches expected feature schema.\")\n",
    "\n",
    "        if \"diagnosis\" in new_data.columns:\n",
    "            X_new = new_data.drop(columns=[\"diagnosis\"])\n",
    "            y_new = new_data[\"diagnosis\"]\n",
    "            st.info(\"Diagnosis column detected â€” evaluation metrics will be computed.\")\n",
    "            # TODO: Run evaluation (accuracy, ROC, calibration, DCA) on X_new, y_new\n",
    "        else:\n",
    "            X_new = new_data\n",
    "            y_new = None\n",
    "            st.warning(\"No diagnosis column found â€” only predictions will be available.\")\n",
    "            # TODO: Run predictions only\n",
    "\n",
    "# -------------------------\n",
    "# View Preprocessed Dataset\n",
    "# -------------------------\n",
    "st.markdown(\"\"\"\n",
    "### View Preprocessed Dataset\n",
    "Below is the structure of the preprocessed dataset used for training and evaluation.\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    pre_df = pd.read_csv(r\"C:\\Users\\yasmine\\Documents\\Portfolio\\DataSciencePortfolio\\Projects\\Breast-Cancer\\data\\preprocessed\\breast_cancer_pruned.csv\")\n",
    "    st.write(\"Preprocessed dataset shape:\", pre_df.shape)\n",
    "    st.dataframe(pre_df.head(10))  # show first 10 rows\n",
    "except Exception as e:\n",
    "    st.error(f\"Could not load preprocessed dataset: {e}\")   \n",
    "\n",
    "# -------------------------\n",
    "# Artifacts\n",
    "# -------------------------\n",
    "st.subheader(\"Artifacts\")\n",
    "st.info(\"Figures and results are displayed directly in the dashboard. Use the download buttons above to export data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhQAUs-Udfoc"
   },
   "source": [
    "# Deployment Guide\n",
    "\n",
    "This section explains how to reuse models outside the dashboard and apply them to new patient data.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Export Model + Threshold Bundle\n",
    "- **Purpose:** Save a trained model with its tuned threshold and feature list for reuse.  \n",
    "- **Contents of the bundle:**\n",
    "  - Model path (location of the serialized model file)\n",
    "  - Model type (e.g., Logistic Regression, Random Forest)\n",
    "  - Tuned threshold value\n",
    "  - Feature list (columns expected in input data)\n",
    "- **Usage:** Exported bundles can be loaded in other environments (e.g., Colab, Streamlit apps) to ensure consistent predictions.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Batch Scoring\n",
    "- **Purpose:** Score multiple new patient records at once.  \n",
    "- **Steps:**\n",
    "  1. Upload a CSV file containing patient features.  \n",
    "  2. Select the model to use for scoring.  \n",
    "  3. The dashboard outputs:\n",
    "     - Predicted probability of malignancy\n",
    "     - Final prediction (Benign/Malignant) based on the tuned threshold\n",
    "  4. Results are saved as a CSV in the artifacts directory for reproducibility.\n",
    "- **Note:** Uploaded CSV must align with the training feature set. Missing columns are automatically filled with default values.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Interactive Prediction (Single Case)\n",
    "- **Purpose:** Test the model on a single patient case interactively.  \n",
    "- **Steps:**\n",
    "  1. Enter values for selected features (defaults are pre-filled with dataset averages).  \n",
    "  2. Choose a model and click **Predict**.  \n",
    "  3. The dashboard displays:\n",
    "     - Probability of malignancy\n",
    "     - Final prediction (Benign/Malignant)\n",
    "- **Optional:** If SHAP is enabled, a local explanation plot shows which features most influenced the prediction.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Artifacts Management\n",
    "- **Purpose:** Ensure reproducibility and easy sharing of results.  \n",
    "- **Artifacts saved include:**\n",
    "  - ROC curves\n",
    "  - Confusion matrices\n",
    "  - Calibration curves\n",
    "  - SHAP plots\n",
    "  - Threshold tuning metrics CSV\n",
    "  - Batch scoring results\n",
    "- **Download options:**\n",
    "  - Individual files (plots, CSVs, models)\n",
    "  - Full ZIP bundle containing all artifacts\n",
    "  \n",
    "  \n",
    "\n",
    " **Key Takeaway:**  \n",
    "This deployment workflow ensures that models trained in the dashboard can be exported, reused, and explained consistently. Batch scoring supports large datasets, while interactive prediction enables case-by-case interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbM9DOiEesMi6iwLRoIv9z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
